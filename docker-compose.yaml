version: '3.9'
services:
  internlm-xcomposer2-4khd-7b:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: internlm
        HF_REPO: internlm-xcomposer2-4khd-7b
        EXAMPLE_FILE: internlm-xcomposer2-4khd-7b_example.py
        PIP_EXTRA: "accelerate bitsandbytes transformers[sentencepiece]"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: internlm-xcomposer2-4khd-7b
    container_name: internlm-xcomposer2-4khd-7b
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  mini-internvl-chat-4b-v1-5:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: OpenGVLab
        HF_REPO: Mini-InternVL-Chat-4B-V1-5
        EXAMPLE_FILE: Mini-InternVL-Chat-4B-V1-5_example.py
        PIP_EXTRA: "peft flash_attn timm transformers[sentencepiece]"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: mini-internvl-chat-4b-v1-5
    container_name: mini-internvl-chat-4b-v1-5
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  internlm-xcomposer2-7b-4bit:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: internlm
        HF_REPO: internlm-xcomposer2-7b-4bit
        EXAMPLE_FILE: internlm-xcomposer2-7b-4bit_example.py
        PIP_EXTRA: "auto_gptq"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: internlm-xcomposer2-7b-4bit
    container_name: internlm-xcomposer2-7b-4bit
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  glm-4v-9b:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: THUDM
        HF_REPO: glm-4v-9b
        EXAMPLE_FILE: glm-4v-9b_example.py
        PIP_EXTRA: "tiktoken accelerate"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: glm-4v-9b
    container_name: glm-4v-9b
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  # Further setup is required for this one, see https://github.com/ShareGPT4Omni/ShareGPT4V
  sharegpt4v-7b:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: Lin-Chen
        HF_REPO: ShareGPT4V-7B
        EXAMPLE_FILE: sharegpt4v-7b_example.py
        PIP_EXTRA: "flash-attn"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: sharegpt4v-7b
    container_name: sharegpt4v-7b
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  minicpm-llama3-v-2_5:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: openbmb
        HF_REPO: MiniCPM-Llama3-V-2_5
        EXAMPLE_FILE: minicpm-llama3-v-2_5_example.py
        PIP_EXTRA: "accelerate bitsandbytes"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: minicpm-llama3-v-2_5
    container_name: minicpm-llama3-v-2_5
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  internvl2-8b:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: OpenGVLab
        HF_REPO: InternVL2-8B
        EXAMPLE_FILE: internvl2-8b_example.py
        PIP_EXTRA: "accelerate bitsandbytes decord flash_attn sentencepiece timm transformers==4.37.2"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: internvl2-8b
    container_name: internvl2-8b
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  internvl2-26b:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: OpenGVLab
        HF_REPO: InternVL2-26B
        EXAMPLE_FILE: internvl2-26b_example.py
        PIP_EXTRA: "accelerate bitsandbytes decord flash_attn sentencepiece timm transformers==4.37.2"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: internvl2-26b
    container_name: internvl2-26b
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true
  #feipengma/WeMM
  wemm-1_2:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        HF_ORG: feipengma
        HF_REPO: WeMM
        EXAMPLE_FILE: wemm-1_2_example.py
        PIP_EXTRA: "peft flash_attn transformers[sentencepiece]"
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    image: wemm-1_2
    container_name: wemm-1_2
    volumes:
      - ./data:/data
    stdin_open: true
    tty: true